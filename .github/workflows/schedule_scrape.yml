name: Scheduled Scrape and Upload

on:
  schedule:
    # 毎朝9時 (JST) に実行 (UTCで0時)
    # cronの書式: 分 時 日 月 曜日
    - cron: '0 0 * * *'
  workflow_dispatch: # 手動実行も可能にする

jobs:
  scrape_and_upload:
    runs-on: windows-latest # Windows環境で実行

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '18'

      - name: Install @types/node
        run: npm install --save-dev @types/node

      - name: Install googleapis
        run: npm install --save-dev googleapis

      - name: Install @googleapis/drive
        run: npm install --save-dev @googleapis/drive

      - name: Install @types/googleapis
        run: npm install --save-dev @types/googleapis

      - name: Install Playwright browsers
        run: npx playwright install

      # google-drive-credentials.json を GitHub Secrets から復元
      - name: Create Google Drive credentials file
        env:
          GOOGLE_DRIVE_CREDENTIALS_JSON: ${{ secrets.GOOGLE_DRIVE_CREDENTIALS_JSON }}
        run: |
          echo "$GOOGLE_DRIVE_CREDENTIALS_JSON" > google-drive-credentials.json
          echo "google-drive-credentials.json created."

      - name: Run scraper script
        # バッチファイルはインタラクティブな入力を求めるため、Actionsでは直接コマンドを実行
        # 必要に応じて引数を追加してください (例: --keyword="your keyword" --max-jobs=50)
        env:
          GITHUB_ACTIONS: true
        run: npm run scrape --skip-chunk-confirm

      # オプション: 生成されたCSVファイルをアーティファクトとして保存
      # dataディレクトリ内の最新のCSVファイルを特定するロジックが必要になる場合があります。
      # ここでは単純にdataディレクトリ全体を保存する例を示します。
      - name: Upload artifact (data directory)
        uses: actions/upload-artifact@v4
        with:
          name: scraped-data
          path: data/
          if-no-files-found: ignore # ファイルがなくてもエラーにしない
